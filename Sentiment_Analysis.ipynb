{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVFnaIhtttyf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
        "from scikitplot.metrics import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.analyticsvidhya.com/blog/2021/06/nlp-sentiment-analysis/"
      ],
      "metadata": {
        "id": "IpSiXuzMtykR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"train.txt\",delimiter=';',names=['text','label'])\n",
        "df_val = pd.read_csv(\"val.txt\",delimiter=';',names=['text','label'])\n",
        "df = pd.concat([df_train,df_val])\n",
        "df.reset_index(inplace=True,drop=True)\n",
        "print(\"Shape of the DataFrame:\",df.shape)\n",
        "print(df.sample(5))"
      ],
      "metadata": {
        "id": "wFIXQO-gtyrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_encoder(df):\n",
        "    df.replace(to_replace =\"surprise\", value =1, inplace=True)\n",
        "    df.replace(to_replace =\"love\", value =1, inplace=True)\n",
        "    df.replace(to_replace =\"joy\", value =1, inplace=True)\n",
        "    df.replace(to_replace =\"fear\", value =0, inplace=True)\n",
        "    df.replace(to_replace =\"anger\", value =0, inplace=True)\n",
        "    df.replace(to_replace =\"sadness\", value =0, inplace=True)"
      ],
      "metadata": {
        "id": "ytEI1GWNuNpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_encoder(df['label'])"
      ],
      "metadata": {
        "id": "g86YMm6cuNra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#object of WordNetLemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "def text_transformation(df_col):\n",
        "    corpus = []\n",
        "    for item in df_col:\n",
        "        new_item = re.sub('[^a-zA-Z]',' ',str(item))\n",
        "        new_item = new_item.lower()\n",
        "        new_item = new_item.split()\n",
        "        new_item = [lm.lemmatize(word) for word in new_item if word not in set(stopwords.words('english'))]\n",
        "        corpus.append(' '.join(str(x) for x in new_item))\n",
        "    return corpus\n",
        "\n",
        "corpus = text_transformation(df['text'])"
      ],
      "metadata": {
        "id": "RkfvtKlXuNwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(1,2))\n",
        "traindata = cv.fit_transform(corpus)\n",
        "X = traindata\n",
        "y = df.label"
      ],
      "metadata": {
        "id": "KTfJRAJ0uNzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'max_features': ('auto','sqrt'),\n",
        "             'n_estimators': [500, 1000, 1500],\n",
        "             'max_depth': [5, 10, None],\n",
        "             'min_samples_split': [5, 10, 15],\n",
        "             'min_samples_leaf': [1, 2, 5, 10],\n",
        "             'bootstrap': [True, False]}"
      ],
      "metadata": {
        "id": "vv_rAGcvuN5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(RandomForestClassifier(),parameters,cv=5,return_train_score=True,n_jobs=-1)\n",
        "grid_search.fit(X,y)\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "y1eRXA97uwCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(432):\n",
        "    print('Parameters: ',grid_search.cv_results_['params'][i])\n",
        "    print('Mean Test Score: ',grid_search.cv_results_['mean_test_score'][i])\n",
        "    print('Rank: ',grid_search.cv_results_['rank_test_score'][i])"
      ],
      "metadata": {
        "id": "lNIoLmlquwHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(max_features=grid_search.best_params_['max_features'],                                  max_depth=grid_search.best_params_['max_depth'],\n",
        "                                  n_estimators=grid_search.best_params_['n_estimators'],                                      min_samples_split=grid_search.best_params_['min_samples_split'],                                    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
        "                                    bootstrap=grid_search.best_params_['bootstrap'])\n",
        "rfc.fit(X,y)"
      ],
      "metadata": {
        "id": "3qhk4GQ_uwNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('test.txt',delimiter=';',names=['text','label'])\n",
        "X_test,y_test = test_df.text,test_df.label\n",
        "#encode the labels into two classes , 0 and 1\n",
        "test_df = custom_encoder(y_test)\n",
        "#pre-processing of text\n",
        "test_corpus = text_transformation(X_test)\n",
        "#convert text data into vectors\n",
        "testdata = cv.transform(test_corpus)\n",
        "#predict the target\n",
        "predictions = rfc.predict(testdata)"
      ],
      "metadata": {
        "id": "KjluPQZTu4XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10,5\n",
        "plot_confusion_matrix(y_test,predictions)\n",
        "acc_score = accuracy_score(y_test,predictions)\n",
        "pre_score = precision_score(y_test,predictions)\n",
        "rec_score = recall_score(y_test,predictions)\n",
        "print('Accuracy_score: ',acc_score)\n",
        "print('Precision_score: ',pre_score)\n",
        "print('Recall_score: ',rec_score)\n",
        "print(\"-\"*50)\n",
        "cr = classification_report(y_test,predictions)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "KkHNrFynu4aA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}